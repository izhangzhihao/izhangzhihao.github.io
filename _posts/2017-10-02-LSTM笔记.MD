---
layout: post
title: LSTM笔记
categories: 机器学习
description: LSTM笔记
---

### 注：本文结合了 [理解长短期记忆(LSTM) 神经网络](https://zhuanlan.zhihu.com/p/24018768) 和 [CS224D](http://cs224d.stanford.edu) 的slide


## 先从RNN开始：

传统的CNN用于图像识别效果很好，但是如果将CNN用于NLP结果就不会太好，尤其对于复杂情况应用CNN很难得到想要的结果。因为`传统的神经网络并不能做到持续记忆`, 而许多情境下我们需要从复杂的上下文中推断出信息，这个时候CNN就不能满足需求了。

先看RNN的结构

<img width="200" src="https://user-images.githubusercontent.com/12044174/31071615-4d3d4bd8-a72a-11e7-8fdd-556c3d15a44d.png" rel="preload" as="image">


其中`A 代表神经网络主体, xt 是网络输入，ht是网络输出，循环结构允许信息从当前输出传递到下一次的网络输入`，所以是一个递归的结构。展开来看是这样的：

![](https://user-images.githubusercontent.com/12044174/31071558-17f36444-a72a-11e7-969c-3635c78239df.png)

<img width="497" src="https://user-images.githubusercontent.com/12044174/31062080-734cf7fc-a6ed-11e7-84b9-75987b19a566.png" rel="preload" as="image">

<img width="380" src="https://user-images.githubusercontent.com/12044174/31062077-71f65628-a6ed-11e7-8de1-d710a77e79c9.png" rel="preload" as="image">

<img width="479" src="https://user-images.githubusercontent.com/12044174/31062076-707d1df4-a6ed-11e7-8f93-8f09c19893b2.png" rel="preload" as="image">

可以看到hiddenState是从过去传到未来的，也就是说RNN会`记住`前世的信息。

### RNN 的缺点：

1. 难以训练：Multiply the same matrix at each time step during forward prop
2. 梯度弥散、梯度爆炸：Multiply the same matrix at each time step during backprop（当需要查找的结果距离当前位置很远时，梯度可能会变得很小）

## LSTM（Long-short-term-memories）

<img width="800" 
src="https://user-images.githubusercontent.com/12044174/31071572-2c1d0d30-a72a-11e7-8f9a-08a8dd9865da.png" rel="preload" as="image">

<img width="800"
src="https://user-images.githubusercontent.com/12044174/31063886-b26d37ee-a6fc-11e7-8323-e8ac85e96125.png" rel="preload" as="image">

1. 首先是`遗忘门`：以x和上一层输出的hidden state为输入，通过sigmod决定遗忘那些数据
2. 然后是`输入门`：以x和上一层输出的hidden state为输入，通过sigmod决定记住那些数据
3. 然后是`输出门`：以x和上一层输出的hidden state为输入，通过sigmod决定输出那些数据
4. 再是`cell`：以x和上一层输出的hidden state为输入，通过sigmod决定记住这一层的那些数据
5. 然后是这一层最终记录什么数据：如果`ft`是0则会忘记以前的全部信息。如果it是0曾会忘记这层的全部信息，然后相加得到这层的最终结果
6. 最后保存这一层的hidden state，即`输出门`乘以经过`tanh`的结果




