---
layout: post
title: 维度表建模小结
categories: [数仓,数据工程,数据建模]
description: 一些个人对维度表建模的思考总结
---


## 什么是维度建模

在数据建模中，通常将包含了业务度量的数据称之为“事实”，而将产生度量的环境称之为“维度”。但是“维度”并非天然存在，而是需要通过业务分析来拆解，将相同的维度数据存储在一个表，这就是“维度表”。维度表可以看做数据分析的视角，比如时间、城市、用户等等，没有维度表的支撑而仅仅针对事实表进行分析的话，结果将相当单薄。可以说事实表就是红花，而维度表就是绿叶，二者缺一不可。

但是，事实表和维度表也不一定是固定不变的，在不同业务团队的视角下，事实表和维度表可能是可以互换的。对于管理订单的团队，订单自然是事实表，而用户是维度表。但是对于负责社区运营的团队，用户也可以是事实表。

## 维度建模在hive的实践

本文将总结集中在hive中进行维度建模的经验和思考。

众所周知，hive的写入和读取都是以分区为单位。在这种情况下，进行维度建模的时候需要考虑些什么才能最大化利用或者说适应于hive的模式呢？
简单思考下大概是这两个问题

* 增量还是全量？增量的具体实现方式是什么？
* 增量的话，如何分区？以什么字段分区？

### 全量还是增量？

考虑到具体的业务和数据情况，不会有一种方式能够适用所有场景，我大致总结如下：

* 假如维度表数据量很小，例如城市的维度表就不会有多少条数据，自然全量覆盖是最佳选择。
* 假如数据量比较大
    * 还需要考虑业务特性，如果每日增量占存量的比例很高（或者说所有的数据都经常更新，例如用户表），在极端情况下还是可能退化到近似全量的结果，增量的意义可以自行判断
    * 如果每日增量的比例不高（或者说只有最新的数据才会更新，而过去的数据都不会或很少更新了，例如订单表），这时可以使用增量的方式

### 如何分区？

分区基本上两种方式，一是通过创建时间分区，二是通过对业务主键取hash来分区。两种方式都较为稳定，但是一对于业务数据略有要求，不一定所有的业务数据都是有create time的，而且会由于业务在特定时间的数据量较大而产生数据倾斜的情况。通过对业务主键取hash来分区（hash结果的前两位），可以得到相对均匀的分区，但是在极端情况下仍然会退化成为近似全量更新（假如需要更新的数据很多，几乎覆盖了所有hash分区。当然另一种办法是改进hash，例如将可能更新的数据都hash到一个分区，但是又掉回了数据倾斜的陷阱... ...）。

两种分区方式各有优略，看官可以根据自己的业务情况判断决策。通过Hive进行分区增量处理拉链表的代码实例可以参考[Sharp ETL的实现](https://github.com/SharpData/SharpETL/blob/main/spark/src/main/scala/com/github/sharpdata/sharpetl/spark/transformation/SCDTransformer.scala)。

## 没有银弹？

这么看下来，似乎没有一种方式可以放之四海而皆准啊。想要给特定情况的维度表进行建模需要的不仅仅是知道概念，还需要多一些经验和思考。简单粗暴的copy其他人或者其他项目的建模结果并不可取。

## 还没结束

如果当前考虑的不是hive的支持，那当然是极好的，无论是通过postgres或者其他RBDMS实现的小型数仓对于create、delete、update单条数据都是有较好的支持。如果是一些RDBMS的魔改版，例如YelloBrick作为魔改版的postgre，对于单条的操作有支持但不是很友好。如果是更现代的存储结构，例如Delta、Hudi、Iceberg这些，单条数据的近实时操作可以说都是他们的最大卖点了，毕竟他们的目标都是替代hive。

甚至Delta内建了[History table](https://docs.databricks.com/delta/history.html)来支持Time travel，听名字就大概知道这是一张保存了历史的表，并且可以通过时间或者版本号来查询当时的数据情况，官方文档说“Each operation that modifies a Delta Lake table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.”。官方的使用sample是：

```sql
SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'
SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123
```

而Hudi也有类似的功能，叫做[Timeline](https://hudi.apache.org/docs/timeline)，官方文档说“Hudi maintains a timeline of all actions performed on the table at different instants of time that helps provide instantaneous views of the table, while also efficiently supporting retrieval of data in the order of arrival”，可以说和Delta异曲同工了，官方的使用[sample](https://hudi.apache.org/docs/next/quick-start-guide/#time-travel-query)是：

```sql
create table hudi_cow_pt_tbl (
  id bigint,
  name string,
  ts bigint,
  dt string,
  hh string
) using hudi
tblproperties (
  type = 'cow',
  primaryKey = 'id',
  preCombineField = 'ts'
 )
partitioned by (dt, hh)
location '/tmp/hudi/hudi_cow_pt_tbl';

insert into hudi_cow_pt_tbl select 1, 'a0', 1000, '2021-12-09', '10';
select * from hudi_cow_pt_tbl;

-- record id=1 changes `name`
insert into hudi_cow_pt_tbl select 1, 'a1', 1001, '2021-12-09', '10';
select * from hudi_cow_pt_tbl;

-- time travel based on first commit time, assume `20220307091628793`
select * from hudi_cow_pt_tbl timestamp as of '20220307091628793' where id = 1;
-- time travel based on different timestamp formats
select * from hudi_cow_pt_tbl timestamp as of '2022-03-07 09:16:28.100' where id = 1;
select * from hudi_cow_pt_tbl timestamp as of '2022-03-08' where id = 1;
```

当然，Iceberg也不会落后，这个功能也叫Time travel，官方[sample](https://iceberg.apache.org/docs/latest/spark-queries/#time-travel)：


```sql
-- time travel to October 26, 1986 at 01:21:00
SELECT * FROM prod.db.table TIMESTAMP AS OF '1986-10-26 01:21:00';

-- time travel to snapshot with id 10963874102873L
SELECT * FROM prod.db.table VERSION AS OF 10963874102873;
```

## 更进一步

实际上如果你一定要使用hive，又不想受限于hive的分区粒度读写。那么还有另一种办法，将Delta、Hudi、Iceberg注册到hive metastore，这样就可以通过hive来读写这张表了。只不过有一个缺点，我查询文档后发现[Iceberg支持通过hive sql进行time travel](https://iceberg.apache.org/docs/latest/hive/#timetravel)，[Hudi也支持通过hive sql进行time travel](https://github.com/apache/hudi/issues/4433)，Delta则[尚不支持](https://github.com/delta-io/connectors/issues/202)。


以上~