---
layout: post
title: 范数和距离
categories: 机器学习
description: 范数、距离和正则化
keywords: 范数, Lp范数, 名氏距离, 明可夫斯基距离, 正则化, 编辑距离, jaccard distance, 余弦相似性, Cosine similarity
---

## 范数

范数是将向量映射到非负值的函数。直观上讲向量`x`的范数表示原点到`x`的距离。一般我们通过范数来衡量一个向量的大小。

Lp范数定义如下：

![L-p norm](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f2d83bfa397bdf021046004b9a365079cab6a22)

其中： p>=1

当p=2时：L2范数称为欧几里得范数(Euclidean norm),它表示从原点出发到向量`x`确定的点的欧几里得距离。平方L2范数也经常用来衡量向量的大小，可以简单地通过`矩阵的转置和矩阵的点积`计算。

当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用L1范数。每当`x`中某个元素从`0`增加到`e`，对应的L1范数也会增加`e`。

![L1 norm](https://wikimedia.org/api/rest_v1/media/math/render/svg/6909908a18e848414a32a6310c5c7fed3f18e7b6)

## 明可夫斯基距离(名氏距离)

p-norm distance：

![p-norm distance](https://wikimedia.org/api/rest_v1/media/math/render/svg/14b24f10bce2d1ceac1b92bf045a9d5c552d9bb8)

p取1或2时的明氏距离是最为常用的，p=2即为欧氏距离(Euclidean distance)，而p=1时则为曼哈顿距离(Taxicab geometry)。当p取无穷时的极限情况下，可以得到切比雪夫距离(Chebyshev distance)：

infinity norm distance
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e1f888a02c22ef38252f7761af834d1882706998)
![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7907acaf88f9ce7974657764586b54090ea99a97)

## 正则化

正则化损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用 L1 正则化的模型建叫做 Lasso 回归，使用 L2 正则化的模型叫做 Ridge 回归（岭回归）。

### L1正则化

L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。

稀疏矩阵指的是很多元素为 0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是 0。 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是 0 或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。

### L2正则化

L2 正则化可以防止模型过拟合（overfitting）。一定程度上，L1 也可以防止过拟合。

拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响。但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响 -- 泛化能力强。

## [Edit distance(编辑距离)](https://zh.wikipedia.org/wiki/編輯距離)

编辑距离是针对二个字符串（例如英文字）的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。编辑距离可以用在自然语言处理中，例如拼写检查可以根据一个拼错的字和其他正确的字的编辑距离，判断哪一个（或哪几个）是比较可能的字。Unix 下的 diff 及 patch 即是利用编辑距离来进行文本编辑对比的例子。

编辑距离有几种不同的定义，差异在可以对字符串进行的处理。

* 在莱文斯坦距离中，可以删除、加入、取代字符串中的任何一个字元，也是较常用的编辑距离定义，常常提到编辑距离时，指的就是莱文斯坦距离。
* 也存在其他编辑距离的定义方式，例如 Damerau-Levenshtein 距离是一种莱文斯坦距离的变种，但允许以单一操作交换相邻的两个字符（称为字符转置），如 AB→BA 的距离是 1（交换）而非 2（先删除再插入、或者两次替换）。
* LCS（最长公共子序列）距离只允许删除、加入字元。
* Jaro 距离只允许字符转置。
* 汉明距离只允许取代字元。

## Jaccard distance

Jaccard相似指数用来度量两个集合之间的相似性，它被定义为两个集合交集的元素个数除以并集的元素个数。

![Jaccard similarity coefficient](https://wikimedia.org/api/rest_v1/media/math/render/svg/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7)

Jaccard距离用来度量两个集合之间的差异性，它是Jaccard的相似系数的补集，被定义为1减去Jaccard相似系数。

![Jaccard distance](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d17a48a5fb6cea57b076200de6edbccbc1c38f9)

## [余弦相似性(Cosine similarity)](https://zh.wikipedia.org/wiki/余弦相似性)

余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。

注意这上下界对任何维度的向量空间中都适用，而且余弦相似性最常用于高维正空间。例如在信息检索中，每个词项被赋予不同的维度，而一个文档由一个向量表示，其各个维度上的值对应于该词项在文档中出现的频率。余弦相似度因此可以给出两篇文档在其主题方面的相似度。
另外，它通常用于文本挖掘中的文件比较。此外，在数据挖掘领域中，会用到它来度量集群内部的凝聚力。

两个向量间的余弦值可以通过使用欧几里得点积公式求出：

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a9f8c962f73f83456742caa95c89970a18a97f2e)

给定两个属性向量，A和B，其余弦相似性θ由点积和向量长度给出，如下所示：

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a8c50526e2cc7aa837477be87eff1ea703f9dec)

这里的 Ai 和 Bi 分别代表向量 A 和 B 的各分量。

```python
def cosine_similarity(vector1, vector2):
    from sklearn.metrics.pairwise import cosine_similarity
    return cosine_similarity(vector1, vector2)

def cosin_distance(vector1, vector2):
    from sklearn.metrics.pairwise import cosine_distances
    return cosine_distances(vector1, vector2)
```